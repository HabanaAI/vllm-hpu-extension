# Welcome to vLLM x Intel Gaudi

<figure markdown="span" style="display: flex; justify-content: center; align-items: center; gap: 10px; margin: auto;">
  <img src="./assets/logos/vllm-logo-text-light.png" alt="vLLM" style="width: 30%; margin: 0;"> x
  <img src="./assets/logos/gaudi-logo.png" alt="Intel-Gaudi" style="width: 30%; margin: 0;">
</figure>

<p style="text-align:center">
</p>

<p style="text-align:center">
<script async defer src="https://buttons.github.io/buttons.js"></script>
<a class="github-button" href="https://github.com/HabanaAI/vllm-fork" data-show-count="true" data-size="large" aria-label="Star">Star</a>
<a class="github-button" href="https://github.com/HabanaAI/vllm-fork/subscription" data-show-count="true" data-icon="octicon-eye" data-size="large" aria-label="Watch">Watch</a>
<a class="github-button" href="https://github.com/HabanaAI/vllm-fork/fork" data-show-count="true" data-icon="octicon-repo-forked" data-size="large" aria-label="Fork">Fork</a>
</p>

vLLM HPU plugin (vllm-hpu) integrates Intel Gaudi accelerators with vLLM to optimize large language model inference.

This plugin follows the [[RFC]: Hardware pluggable](https://github.com/vllm-project/vllm/issues/11162) and [[RFC]: Enhancing vLLM Plugin Architecture](https://github.com/vllm-project/vllm/issues/19161) principles, providing a modular interface for Intel Gaudi hardware.

Learn more:

ðŸ“š [Intel Gaudi Documentation](https://docs.habana.ai/en/v1.21.1/index.html)  
ðŸš€ [vLLM Plugin System Overview](https://docs.vllm.ai/en/latest/design/plugin_system.html)