---
title: Quantization
---
[](){ #quantization-index }

Quantization trades off model precision for smaller memory footprint, allowing large models to be run on a wider range of devices. The Intel Gaudi Backend supports following quantization backends:

- [Intel Neural Compressor](inc.md)
- [Auto_Awq](auto_awq.md)
- [Gptqmodel](gptqmodel.md)
